{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8029736",
   "metadata": {},
   "source": [
    "# 8.1 Основные понятия Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53722fdb",
   "metadata": {},
   "source": [
    "Если вам показалось, что Spark во всем лучше MapReduce, то прочитайте [комментарий](https://stepic.org/lesson/%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D0%BD%D1%8B%D0%B5-%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%B8%D1%8F-Spark-20975/step/3?course=Hadoop-%D0%A1%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0-%D0%B4%D0%BB%D1%8F-%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8-%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%B8%D1%85-%D0%BE%D0%B1%D1%8A%D0%B5%D0%BC%D0%BE%D0%B2-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85&discussion=160873&reply=160991) от автора. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677c896",
   "metadata": {},
   "source": [
    "<img src=\"slides/le08/Spark/Spark-02.png\" title=\"Spark\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "Spark является некоторым этапом развития парадигмы MapReduce, т.е. MapReduce является частным случаем Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba18c9a",
   "metadata": {},
   "source": [
    "## Недостатки MapReduce\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-03.png\" title=\"Недостатки MapReduce\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "В MapReduce нет эффективных примитивов для работы с общими данными. Например, делая итеративный алгоритм, где каждая итерация считывает данные из HDFS, обрабатывает их и записывает их снова в HDFS. Так происходит на каждой итерации. HDFS не явлется эффективным примитивом для передачи таких данных (не забываем про репликацию для надежности).\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-04.png\" title=\"Недостатки MapReduce\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "Все задачи реализуются с помощью шаблона Map + Reduce. Крайний случай: только Map.  \n",
    "Из слайда видно, что, если нам потребуется реализовать 3 фазы Reduce, то по факту мы получаем задачу с 3 парами Map + Reduce, в 2 последних парах из 3 которых этап Map практически ничего не будет делать (только зачитывать данные и отправлять на Reducer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be94c36",
   "metadata": {},
   "source": [
    "## Ключевые понятия Spark\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-05.png\" title=\"Ключевые понятия MapReduce\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "1 ряд: схема итерации MapReduce задач. Все они обмениваются данными с помощью HDFS.  \n",
    "2 ряд: в Spark применяется подход In-Memory Data Processing and Sharing (данные, которые будут обрабатываться хранятся в памяти).\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-06.png\" title=\"Ключевые понятия MapReduce\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "Пример: есть входные данные в HDFS. Есть несколько Mapreduce задач, каждая из которых считывает эти данные. Получается, что одни и те же данные будут зачитаны в разных MapReduce задачах, что не эффективно.\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-07.png\" title=\"Ключевые понятия MapReduce\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "Пример: в Spark видится более эффективное решение - добавление промежуточного шага, данные в котором хранятся в памяти. Данные из HDFS считываются единожды."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e8fc5",
   "metadata": {},
   "source": [
    "## Не RAM, a RDD\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-08.png\" title=\"RDD\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "Resilient Distributed Datasets (RDD) - это абстрактное представление распределенной памяти. **В Spark программа описывается, как некоторые функции, которые преобразуют одни RDD (один набор данных) в другие RDD (другой набор данных)**. Когда RDD создается и к ней применяется какая-то функция, сама RDD не изменяется, т.е. создается новое RDD, а не изменяется то RDD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd0c69",
   "metadata": {},
   "source": [
    "## Программная модель Spark\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-09.png\" title=\"Программная модель Spark\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "Любую программу Spark можно представить в виде графа. \n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-10.png\" title=\"Программная модель Spark\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "Комбинацией data source + operator + data sink (результат)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b73907",
   "metadata": {},
   "source": [
    "# 8.2 Операторы Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b9535",
   "metadata": {},
   "source": [
    "<img src=\"slides/le08/Spark/Spark-11.png\" title=\"Операторы Spark\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "2 типа RDD операторов:\n",
    "- transformations (одни RDD -> другие RDD)\n",
    "- actions (вычисление над RDD -> в файл, хранилище)\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-12.png\" title=\"Операторы Spark\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae03f8",
   "metadata": {},
   "source": [
    "## RDD transformations\n",
    "\n",
    "Примеры на языке Scala.\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-13.png\" title=\"Spark RDD transformations\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-14.png\" title=\"Spark RDD transformations\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-15.png\" title=\"Spark RDD transformations\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-16.png\" title=\"Spark RDD transformations\" width=\"600\" height=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b45a9",
   "metadata": {},
   "source": [
    "## RDD Actions\n",
    "\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-18.png\" title=\"Spark RDD actions\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-19.png\" title=\"Spark RDD actions\" width=\"600\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de699379",
   "metadata": {},
   "source": [
    "## SparkContext\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-20.png\" title=\"SparkContext\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "Для работы со Spark нужно создать Spark context(переменная `sc`).  \n",
    "Если нам потребуется создать переменную в языке, то нам нужно будеть указать, где будет располагаться `master` (локально или `spark://host:port` - будет запущена программа на кластере со spark), `appName` - название программы и доп параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86917b",
   "metadata": {},
   "source": [
    "## Пример\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-22.png\" title=\"SparkContext\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "- Есть большой файл в HDFS. \n",
    "- С помощью SparkContext загружаем данные из этого файла в RDD под названием `file`.\n",
    "- К этому RDD применяем оператор `filter` по условию: строки должны содержать `\"MAIL\"`. Результат будет новый RDD.\n",
    "- На новом RDD вызываем функцию `cache`, чтобы определить, как данные будут храниться. Мы говоиим, чтоб по-максимуму данные хранились в RAM, чтобы следующие действия выполнялись быстро.\n",
    "- Далее применяем функцию `map` и говорим, что каждую строку мы преобразуем к 1. Получаем новый RDD, состоящий из одних единиц.\n",
    "- Применяем `reduce` ко всем элементам.\n",
    "\n",
    "Можно всё сделать и короче (вариант в 2 строки)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be021cc2",
   "metadata": {},
   "source": [
    "## Пример 1\n",
    "\n",
    "`latin.txt`:\n",
    "```txt\n",
    "Veni, vidi, vici.\n",
    "Alea jacta est.\n",
    "Gutta cavat lapidem.\n",
    "```\n",
    "\n",
    "Scala:\n",
    "```scala\n",
    "val text = sc.textFile(\"latin.txt\")\n",
    "\n",
    "\n",
    "text.flatMap(line => line.split(\" \"))                 \n",
    "                 .map(x => (x, 1))\n",
    "                 .count()\n",
    "```\n",
    "\n",
    "Python:\n",
    "```python\n",
    "# Python 3.8\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "file = \"latin.txt\"\n",
    "\n",
    "sc = SparkContext(\"local\", \"First App\")\n",
    "cache_data = sc.textFile(file).cache()\n",
    "\n",
    "rdd = cache_data.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1)).count()\n",
    "\n",
    "print(rdd)  # 9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce7084c",
   "metadata": {},
   "source": [
    "## Пример 2\n",
    "\n",
    "Scala:\n",
    "```scala\n",
    "val nums=sc.parallelize(Array(100, 1000, 10000))\n",
    "nums.flatMap(x => Range(1, x, 1)).take(3)\n",
    "```\n",
    "\n",
    "Python:\n",
    "```python\n",
    "# Python 3.8\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "sc = SparkContext(\"local\", \"First App\")\n",
    "data = sc.parallelize([100, 1000, 1000])\n",
    "\n",
    "rdd = data.flatMap(lambda x: range(1, x, 1)).take(3)\n",
    "\n",
    "print(rdd)  # [1,2,3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f0a903",
   "metadata": {},
   "source": [
    "## Spark shared variables\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-23.png\" title=\"Spark shared variables\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "Возникает необходимость использовать общие переменные, которые могут использовать разные workers. Могут читать, а могут изменять переменные.  \n",
    "- Broadcast variable можно только считывать\n",
    "- Accumulators можно только изменять\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-24.png\" title=\"Spark shared variables\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "<img src=\"slides/le08/Spark/Spark-25.png\" title=\"Spark shared variables\" width=\"600\" height=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3e0e5",
   "metadata": {},
   "source": [
    "# 8.3 Фреймворк Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad7ca2",
   "metadata": {},
   "source": [
    "<img src=\"slides/le08/Spark_engine/Spark_engine-02.png\" title=\"Framework Spark\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "<img src=\"slides/le08/Spark_engine/Spark_engine-03.png\" title=\"Framework Spark\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "<img src=\"slides/le08/Spark_engine/Spark_engine-04.png\" title=\"Framework Spark\" width=\"600\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4da46b",
   "metadata": {},
   "source": [
    "## Lineage\n",
    "\n",
    "<img src=\"slides/le08/Spark_engine/Spark_engine-05.png\" title=\"Spark Lineage\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "Lineage есть у каждого RDD. Можно примерно перевести как \"линия жизни\". **Цепочка трансформаций из одного RDD -> другой RDD и есть Lineage**. Справа пример Lineage для RDD `ones`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94dbd9",
   "metadata": {},
   "source": [
    "## Зависимости между RDD\n",
    "\n",
    "При (parent) RDD -> RDD (child) существует 2 типа зависимостей преобразлования:\n",
    "- narrow (узкое)\n",
    "- wide (широкое)\n",
    "\n",
    "<img src=\"slides/le08/Spark_engine/Spark_engine-07.png\" title=\"Зависимости между RDD\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "**Narrow**: когда **parent RDD partition** участвовала в образовании **child RDD partition** только **1 раз**. (`map`, `filter`, `union`, `join`). Это позволяет запускать это преобразование на одной ноде кластера. Можно даже запускать последовательность таких операций на одной ноде.\n",
    "\n",
    "<img src=\"slides/le08/Spark_engine/Spark_engine-08.png\" title=\"Зависимости между RDD\" width=\"600\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59273a19",
   "metadata": {},
   "source": [
    "## Архитектура запуска программы Spark\n",
    "\n",
    "<img src=\"slides/le08/Spark_engine/Spark_engine-09.png\" title=\"Архитектура запуска программы Spark\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "- Некоторая программа зачитывает данные из HDFS и получает 3 различных RDD (синие блоки).\n",
    "- RDD_1 -map-> RDD_4 (red)\n",
    "- RDD_2 -map-> RDD_5 (green)\n",
    "- RDD_5 +union+ RDD_3 -> RDD_6 (purple)\n",
    "- RDD_4 +join+ RDD_6 -> RDD_7\n",
    "- Выполняем action `save` и записываем в HDFS.\n",
    "\n",
    "Операторы transformations являются ленивыми операторами, когда мы их выполняем - никакого реального преобразования не происходит, но как только мы сталкиваемся с оператором action (save), мы должны уже выполнить некоторые преобразованяи данных.\n",
    "\n",
    "- Мы сделали action save для RDD_7. У RDD_7 есть некий lineage. Более того, операторы `map`, `union` имеют narrow зависимость, а вот `join` уже wide зависимость.\n",
    "- Scheduler все narrow преобразования объединяет в одни stage (stage 1, stage 2, stage 3).\n",
    "- Планировщик будет постоянно смотреть какие RDD partitions еще не готовы и для каких нужно запустить процесс."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1f56e",
   "metadata": {},
   "source": [
    "## Spark Memory Manager\n",
    "\n",
    "<img src=\"slides/le08/Spark_engine/Spark_engine-12.png\" title=\"Spark Memory Manager\" width=\"600\" height=\"600\"/>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
